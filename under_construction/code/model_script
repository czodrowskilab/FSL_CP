import numpy as np
import pandas as pd
import xgboost as xgb
import sklearn as sk

from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.model_selection import RandomizedSearchCV
from sklearn import metrics
from sklearn.metrics import roc_auc_score
#from sklearn.metrics import RocCurveDisplay



#################################INITIALISATION#################################
################################################################################



#list of asssays
ls_assay = ["688267", "600886", "737826", "737824_1", "737825", "1495405", "737053", "737400",
 "736947", "752347", "752496", "752509", "752594", "809095", "845173", "845196", "954338", "845206"]

#hyperparameters 
parameters = {
 'learning_rate': [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5],
 'max_depth': np.arange(1, 11, 2),
 'n_estimators': np.arange(50, 550, 50),
 'subsample': [0.5, 1]
 }



output = {'ASSAY_ID':[ls_assay],8:[],16:[],32:[],64:[],96:[]}



#list of support set sizes
ls_sss = [8, 16, 32, 64, 96]

#total numbers of iterations that are done 
total_iter_range = 100

#size of the sample for looking for the most commonly used parameters of a given support set
param_sample_size = 20

#size of iterations left to complete sample size for science
iter = total_iter_range - param_sample_size



################################################################################
################################################################################


###################################FUNCTIONS####################################
################################################################################



#function for saving best parameters as json file
def param_json(df,iter_ass,iter_sss):
    json_name = 'best_params_assay_' + ls_assay[iter_ass] + '_sss_' + ls_sss[iter_sss]
    df.mode().iloc[0,:].to_json(path_or_buf=f'../under_construction/data_output/{json_name}.json')



#function for adding auc mean and std to output dict
def output_dict(auc_df, iter_sss, dict):
    m = np.mean(auc_df[iter_sss].to_numpy())
    s = np.std(auc_df[iter_sss].to_numpy())
    dict[ls_sss[iter_sss]].append(str(m) + '+/-' + str(s))
    #return dict



################################################################################
################################################################################



#building the model with xgboost
model = XGBClassifier(objective='binary:logistic')




#initial for-loop that starts process for every assay we chose
for i in range(len(ls_assay)):
    name = ls_assay[i]

    file_name = name + '.csv'
    print(file_name)
    df_testing = pd.read_csv(f'under_construction/data_output/{file_name}')          #'''LABEL MUST BE IN COLUMN 1 !!!'''



    #initialisation of the training-set
    X = df_testing.iloc[:,1:]#.to_numpy()

    y = df_testing.iloc[:,0].astype(int).to_numpy()


    #for-loop for every support set size
    for j in range(len(ls_sss)):

        df_param = pd.DataFrame(columns=parameters)#reset dataframe of the best parameters after each finished  loop
        df_auc = pd.DataFrame(columns=ls_sss) #reset the auc dataframe

        for k in range(total_iter_range):
            


           #support set
            X_rest, X_support, y_rest, y_support = train_test_split(X, y, test_size=ls_sss[j], stratify=y)

            #query set
            X_unused, X_query, y_unused, y_query = train_test_split(X_rest, y_rest, test_size=32, stratify=y)

            '''chosen_assay_df_2, support_set_df, _unused1, label_support = train_test_split(chosen_assay_df, chosen_assay_df['LABEL'], 
            test_size=self.support_set_size, stratify=chosen_assay_df['LABEL'])
            _unused_2, query_set_df, _unused3, label_query = train_test_split(
            chosen_assay_df_2, chosen_assay_df_2['LABEL'], test_size=self.query_set_size, stratify=chosen_assay_df_2['LABEL'])'''



            #randomsearchcv
            rscv = RandomizedSearchCV(model, parameters, random_state=7)#n_jobs -> -1 

            search = rscv.fit(X_support,y_support)

            params = search.best_params_

            #adding the best parameters of the k-th run to the parameter dataframe
            #temporary = pd.DataFrame(params,index=[k])
            #df_param = pd.concat([temporary,df_param], ignore_index=True)          #'''wird der richtige index wegwn ignore_index assigned?'''


    

            y_pred = search.predict(X_query)

            #evaluating the model with auc metric

            #fpr,tpr,threshholds = metrics.roc_curve(y_query,y_pred)
            auc = sk.metrics.roc_auc_score(y_query,y_pred)
            #auc= metrics.auc(fpr,tpr)



            #adding auc value to auc datafrae
            df_auc.at[k,ls_sss[j]] = auc
            

        output_dict(df_auc, j, output)


        #saving best parameters as json file
        #param_json(df_param,i,j)
        


        '''#model with before determined best parameters
        model_80 = XGBClassifier(
         objective='binary:logistic', learning_rate=df_param.iloc[0,0],max_depth=df_param.iloc[0,1],
         n_estimators=df_param.iloc[0,2],subsample=df_param.iloc[0,3]
         )

        for l in range(iter):



            #support set
            X_trts, X_support, y_trts, y_support = train_test_split(X, y, test_size=ls_sss[j], stratify=y)

            #query set
            X_trts2, X_query, y_trts2, y_query = train_test_split(X, y, test_size=32, stratify=y)
            
            
            pred = model_80.fit(X_support,y_support)

            y_pred = search.predict(X_query)



            #evaluating the model with auc metric
            fpr,tpr,threshholds = metrics.roc_curve(y_query,y_pred)
            #auc = sk.metrics.roc_auc_score(y_tts,y_pred)
            auc= metrics.auc(fpr,tpr)



            #adding auc value to auc datafrae
            df_auc.at[k+l,ls_sss[j]] = auc #k+l richtig?'''
            

    #saving the auc dataframe for EACH assay
    df_auc_name = 'AUC_ASSAY' + ls_assay[i]
    
    df_auc.to_csv(f'../under_construction/data_output/{df_auc_name}.csv')

    #output of mean and std of auc
    #output['ASSAY_ID'] = ls_assay[i]
   


print('end')
