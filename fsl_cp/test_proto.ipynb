{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import Omniglot\n",
    "from torchvision.models import resnet18\n",
    "from tqdm import tqdm\n",
    "from utils.models.protonet import ProtoNet as PrototypicalNetworks\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "\n",
    "# NB: background=True selects the train set, background=False selects the test set\n",
    "# It's the nomenclature from the original paper, we just have to deal with it\n",
    "\n",
    "train_set = Omniglot(\n",
    "    root=\"/home/son.ha/FSL_CP/data\",\n",
    "    background=True,\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.Grayscale(num_output_channels=3),\n",
    "            transforms.RandomResizedCrop(image_size),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    ),\n",
    "    download=True,\n",
    ")\n",
    "test_set = Omniglot(\n",
    "    root=\"/home/son.ha/FSL_CP/data\",\n",
    "    background=False,\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            # Omniglot images have 1 channel, but our model will expect 3-channel images\n",
    "            transforms.Grayscale(num_output_channels=3),\n",
    "            transforms.Resize([int(image_size * 1.15), int(image_size * 1.15)]),\n",
    "            transforms.CenterCrop(image_size),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    ),\n",
    "    download=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Flatten(start_dim=1, end_dim=-1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "class PrototypicalNetworks(nn.Module):\n",
    "    def __init__(self, backbone: nn.Module):\n",
    "        super(PrototypicalNetworks, self).__init__()\n",
    "        self.backbone = backbone\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        support_images: torch.Tensor,\n",
    "        support_labels: torch.Tensor,\n",
    "        query_images: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Predict query labels using labeled support images.\n",
    "        \"\"\"\n",
    "        # Extract the features of support and query images\n",
    "        z_support = self.backbone.forward(support_images)\n",
    "        z_query = self.backbone.forward(query_images)\n",
    "\n",
    "        # Infer the number of different classes from the labels of the support set\n",
    "        n_way = len(torch.unique(support_labels))\n",
    "        # Prototype i is the mean of all instances of features corresponding to labels == i\n",
    "        z_proto = torch.cat(\n",
    "            [\n",
    "                z_support[torch.nonzero(support_labels == label)].mean(0)\n",
    "                for label in range(n_way)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Compute the euclidean distance from queries to prototypes\n",
    "        dists = torch.cdist(z_query, z_proto)\n",
    "\n",
    "        # And here is the super complicated operation to transform those distances into classification scores!\n",
    "        scores = -dists\n",
    "        return scores\n",
    "'''\n",
    "\n",
    "convolutional_network = resnet18(pretrained=True)\n",
    "convolutional_network.fc = nn.Flatten()\n",
    "print(convolutional_network)\n",
    "\n",
    "model = PrototypicalNetworks(convolutional_network).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_average(value_list, window: int) -> float:\n",
    "    \"\"\"\n",
    "    Computes the average of the latest instances in a list\n",
    "    Args:\n",
    "        value_list: input list of floats (can't be empty)\n",
    "        window: number of instances to take into account. If value is 0 or greater than\n",
    "            the length of value_list, all instances will be taken into account.\n",
    "    Returns:\n",
    "        average of the last window instances in value_list\n",
    "    Raises:\n",
    "        ValueError: if the input list is empty\n",
    "    \"\"\"\n",
    "    if len(value_list) == 0:\n",
    "        raise ValueError(\"Cannot perform sliding average on an empty list.\")\n",
    "    return np.asarray(value_list[-window:]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "from typing import List, Tuple\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class FewShotDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Abstract class for all datasets used in a context of Few-Shot Learning.\n",
    "    The tools we use in few-shot learning, especially TaskSampler, expect an\n",
    "    implementation of FewShotDataset.\n",
    "    Compared to PyTorch's Dataset, FewShotDataset forces a method get_labels.\n",
    "    This exposes the list of all items labels and therefore allows to sample\n",
    "    items depending on their label.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def __getitem__(self, item: int) -> Tuple[Tensor, int]:\n",
    "        raise NotImplementedError(\n",
    "            \"All PyTorch datasets, including few-shot datasets, need a __getitem__ method.\"\n",
    "        )\n",
    "\n",
    "    @abstractmethod\n",
    "    def __len__(self) -> int:\n",
    "        raise NotImplementedError(\n",
    "            \"All PyTorch datasets, including few-shot datasets, need a __len__ method.\"\n",
    "        )\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_labels(self) -> List[int]:\n",
    "        raise NotImplementedError(\n",
    "            \"Implementations of FewShotDataset need a get_labels method.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List, Tuple, Iterator\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Sampler\n",
    "\n",
    "\n",
    "class TaskSampler(Sampler):\n",
    "    \"\"\"\n",
    "    Samples batches in the shape of few-shot classification tasks. At each iteration, it will sample\n",
    "    n_way classes, and then sample support and query images from these classes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: FewShotDataset,\n",
    "        n_way: int,\n",
    "        n_shot: int,\n",
    "        n_query: int,\n",
    "        n_tasks: int,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset: dataset from which to sample classification tasks. Must have a field 'label': a\n",
    "                list of length len(dataset) containing containing the labels of all images.\n",
    "            n_way: number of classes in one task\n",
    "            n_shot: number of support images for each class in one task\n",
    "            n_query: number of query images for each class in one task\n",
    "            n_tasks: number of tasks to sample\n",
    "        \"\"\"\n",
    "        super().__init__(data_source=None)\n",
    "        self.n_way = n_way\n",
    "        self.n_shot = n_shot\n",
    "        self.n_query = n_query\n",
    "        self.n_tasks = n_tasks\n",
    "\n",
    "        self.items_per_label = {}\n",
    "        for item, label in enumerate(dataset.get_labels()):\n",
    "            if label in self.items_per_label.keys():\n",
    "                self.items_per_label[label].append(item)\n",
    "            else:\n",
    "                self.items_per_label[label] = [item]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.n_tasks\n",
    "\n",
    "    def __iter__(self) -> Iterator[List[int]]:\n",
    "        for _ in range(self.n_tasks):\n",
    "            yield torch.cat(\n",
    "                [\n",
    "                    # pylint: disable=not-callable\n",
    "                    torch.tensor(\n",
    "                        random.sample(\n",
    "                            self.items_per_label[label], self.n_shot + self.n_query\n",
    "                        )\n",
    "                    )\n",
    "                    # pylint: enable=not-callable\n",
    "                    for label in random.sample(self.items_per_label.keys(), self.n_way)\n",
    "                ]\n",
    "            ).tolist()\n",
    "\n",
    "    def episodic_collate_fn(\n",
    "        self, input_data: List[Tuple[Tensor, int]]\n",
    "    ) -> Tuple[Tensor, Tensor, Tensor, Tensor, List[int]]:\n",
    "        \"\"\"\n",
    "        Collate function to be used as argument for the collate_fn parameter of episodic\n",
    "            data loaders.\n",
    "        Args:\n",
    "            input_data: each element is a tuple containing:\n",
    "                - an image as a torch Tensor\n",
    "                - the label of this image\n",
    "        Returns:\n",
    "            tuple(Tensor, Tensor, Tensor, Tensor, list[int]): respectively:\n",
    "                - support images,\n",
    "                - their labels,\n",
    "                - query images,\n",
    "                - their labels,\n",
    "                - the dataset class ids of the class sampled in the episode\n",
    "        \"\"\"\n",
    "\n",
    "        true_class_ids = list({x[1] for x in input_data})\n",
    "\n",
    "        all_images = torch.cat([x[0].unsqueeze(0) for x in input_data])\n",
    "        all_images = all_images.reshape(\n",
    "            (self.n_way, self.n_shot + self.n_query, *all_images.shape[1:])\n",
    "        )\n",
    "        # pylint: disable=not-callable\n",
    "        all_labels = torch.tensor(\n",
    "            [true_class_ids.index(x[1]) for x in input_data]\n",
    "        ).reshape((self.n_way, self.n_shot + self.n_query))\n",
    "        # pylint: enable=not-callable\n",
    "\n",
    "        support_images = all_images[:, : self.n_shot].reshape(\n",
    "            (-1, *all_images.shape[2:])\n",
    "        )\n",
    "        query_images = all_images[:, self.n_shot :].reshape((-1, *all_images.shape[2:]))\n",
    "        support_labels = all_labels[:, : self.n_shot].flatten()\n",
    "        query_labels = all_labels[:, self.n_shot :].flatten()\n",
    "\n",
    "        return (\n",
    "            support_images,\n",
    "            support_labels,\n",
    "            query_images,\n",
    "            query_labels,\n",
    "            true_class_ids,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_WAY = 5  # Number of classes in a task\n",
    "N_SHOT = 5  # Number of images per class in the support set\n",
    "N_QUERY = 10  # Number of images per class in the query set\n",
    "N_EVALUATION_TASKS = 100\n",
    "\n",
    "# The sampler needs a dataset with a \"get_labels\" method. Check the code if you have any doubt!\n",
    "test_set.get_labels = lambda: [\n",
    "    instance[1] for instance in test_set._flat_character_images\n",
    "]\n",
    "test_sampler = TaskSampler(\n",
    "    test_set, n_way=N_WAY, n_shot=N_SHOT, n_query=N_QUERY, n_tasks=N_EVALUATION_TASKS\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_set,\n",
    "    batch_sampler=test_sampler,\n",
    "    num_workers=12,\n",
    "    pin_memory=True,\n",
    "    collate_fn=test_sampler.episodic_collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2849717/1999250935.py:59: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  for label in random.sample(self.items_per_label.keys(), self.n_way)\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    example_support_images,\n",
    "    example_support_labels,\n",
    "    example_query_images,\n",
    "    example_query_labels,\n",
    "    example_class_ids,\n",
    ") = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2849717/1999250935.py:59: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  for label in random.sample(self.items_per_label.keys(), self.n_way)\n",
      "100%|██████████| 100/100 [00:01<00:00, 93.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model tested on 100 tasks. Accuracy: 86.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_on_one_task(\n",
    "    support_images: torch.Tensor,\n",
    "    support_labels: torch.Tensor,\n",
    "    query_images: torch.Tensor,\n",
    "    query_labels: torch.Tensor,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns the number of correct predictions of query labels, and the total number of predictions.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        torch.max(\n",
    "            model(support_images.cuda(), support_labels.cuda(), query_images.cuda())\n",
    "            .detach()\n",
    "            .data,\n",
    "            1,\n",
    "        )[1]\n",
    "        == query_labels.cuda()\n",
    "    ).sum().item(), len(query_labels)\n",
    "\n",
    "\n",
    "def evaluate(data_loader: DataLoader):\n",
    "    # We'll count everything and compute the ratio at the end\n",
    "    total_predictions = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    # eval mode affects the behaviour of some layers (such as batch normalization or dropout)\n",
    "    # no_grad() tells torch not to keep in memory the whole computational graph (it's more lightweight this way)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for episode_index, (\n",
    "            support_images,\n",
    "            support_labels,\n",
    "            query_images,\n",
    "            query_labels,\n",
    "            class_ids,\n",
    "        ) in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "\n",
    "            correct, total = evaluate_on_one_task(\n",
    "                support_images, support_labels, query_images, query_labels\n",
    "            )\n",
    "\n",
    "            total_predictions += total\n",
    "            correct_predictions += correct\n",
    "\n",
    "    print(\n",
    "        f\"Model tested on {len(data_loader)} tasks. Accuracy: {(100 * correct_predictions/total_predictions):.2f}%\"\n",
    "    )\n",
    "\n",
    "\n",
    "evaluate(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "10316af22c25afef02586b059330728af6980e41a2ff253fb3fe115ef68c8368"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
